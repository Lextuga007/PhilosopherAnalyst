[
  {
    "path": "posts/2021-11-24-losing-the-plots/",
    "title": "Losing the plots",
    "description": "When losing my plotting coding turned into a learning opportunity.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-11-24",
    "categories": [],
    "contents": "\r\nDon’t Panic\r\nI started the day thinking my task would be easy. I had been asked to update a report I’d created a couple of years ago and although I had a week to do it, much longer than such requests can often be, I really wanted to help get this data together quickly. The report had previously been used, I was told, to make a case for better CCTV equipment. That case had been won and now patients and staff had equipment had could provide evidence validate complaints and support prosecutions. Who wouldn’t want to pull out the stops for that kind of feedback!\r\nIncluding code in Rmarkdown\r\nThe day didn’t go as planned. I’d got the old (html) report but I could not remember where I’d saved the coding files. If I’d only known version control when I’d first written it, it would be in the Git/VSTS.\r\nEven without that, I wish I’d known that putting the following into the top YAML:\r\noutput:\r\n  html_document:\r\n    code_download: yes\r\n    \r\nwould produce a little button in the top right hand corner of the report from which anyone, including me importantly, could get the code. Instead, whilst I searched for my code, I also started looking at reproducing the whole document.\r\nFinding SPC charts with signals\r\nThe re-coding wasn’t going so well though. I’d found the data I needed but I couldn’t for the life of me remember how to get just the SPC charts for wards/incidents only with signals as I’d done originally:\r\n\r\n\r\n[1] Female\r\nLevels: Female\r\n\r\nI knew that [@_johnmackintosh](https://twitter.com/_johnmackintosh) had written the original blog that I’d used to do this but it was on a different site to his blog plus he’s build two great packages (runcharter and spccharter) that I really should be using but my panic was rising and I couldn’t really think straight to do anything that constructive!\r\nBut then, suddenly, I just found it.\r\nMicrosoft explorer search of Rmarkdown\r\nStrangely though, I couldn’t find it using Microsoft search, even in the very folder that I knew it was in.\r\nAs I had a complaint to make, of course I turned to Twitter:\r\nImage of tweet saying: “I learned today that MS explorer search is not my friend. It can’t find text from Rmarkdown so I couldn’t find my files. I don’t have these problems with GitHub and yet… Microsoft owns GitHub”and I really should have moaned sooner as I got great responses from people on how and why to resolve this!\r\nSolutions suggested were:\r\n\r\ngrep probably works on Git Bash in Windows I’m guessing?\r\n\r\n[@ChrisBeeley](https://twitter.com/ChrisBeeley) and [@ERDonnachie](https://twitter.com/ERDonnachie) confirmed this does work\r\n\r\nYou probably need to change the indexing options in explorer to include Rmd files. Link\r\n\r\n[@TomJemmett](https://twitter.com/tomjemmett)\r\n\r\nYou can search cloned repositories locally using ‘git-grep’. If you have Linux bash on Win, you can also try ‘grep -r …’ and qualify it however you need (don’t do the entire HDD or it’ll take forever). See the docs. If you don’t have bash, you should Link\r\n\r\n[@Pouriaaa](https://twitter.com/Pouriaaa)\r\nNow I know that I will forget this and I’ll never find it again in Twitter so I’ve got it right here for that future me.\r\nPlus if anyone is reading this blog, do follow these people as they are just the best!\r\nTime on my hands\r\nNow that I had my code it was a ridiculously quick thing to update it to the new data and also change my horrific naming conventions which, quite frankly, were all over the place.\r\nWith that done, I shared the report and got instant feedback:\r\n\r\nThank you for this! I think this report will do the trick Any chance you could do this by individual wards?\r\n\r\nNow, the wards in question numbered just 10, but I had presented the data 4 ways. I’d created functions for the charts but I’d still typed out a title for each ward and then the function code. Doing that 40 times is quite a bit of typing.\r\nAutomatic tabs in Rmarkdown\r\nMy solution was therefore to move from functions that are run individually to running it through a loop, but I also particularly wanted Rmarkdown tabs so that the overall Rmarkdown report isn’t too long. Scrolling down an html page through 40 charts is about as much fun as typing out the titles for them.\r\nAs I do under such circumstances of “is this even possible?” I asked the question of Google and got a Stackoverflow answer. This looked really good but didn’t work for my {qicharts2} charts.\r\nA bit more googling and lo, it appeared to be a problem with the code being base R specific.\r\nA bit of moving code about and I eventually got:\r\n\r\n\r\n\r\nThis code won’t run on its own but this one in the NHS-R Community GitHub Demos and How Tos does.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-24-losing-the-plots/losing-the-plots_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-11-24T18:20:53+00:00",
    "input_file": "losing-the-plots.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-02-creating-a-caseload-over-time-in-sql/",
    "title": "Creating a caseload over time in SQL",
    "description": "How to count referrals/patients/anything between two dates in a given period of time.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-05-02",
    "categories": [
      "SQL"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Photo of pink tulip with frilly ends of petals\r\n\r\n\r\n\r\nI wrote a similar blog, with almost the same title and everything, but with R and now SQL gets its time to shine! I said in the previous blog that I’d spent a long time puzzling over the problem of counts over a period of time, particularly for referrals or caseload; in truth, most of that problem solving was for SQL and the R code was based on the underlying theory.\r\nThe blog referred to these scenarios of open referrals/caseload in a given period:\r\n\r\n1\r\nThe methodology for R was different because I could use specific verbs: complete() and seq.Date() to create the observations/rows between dates but these don’t exist (that I know of) in SQL. To do the same thing in SQL this needs to be done using a JOIN.\r\nCode solution\r\nCreate some data\r\nI’ve coded with SQL for nearly a decade and still struggle with reproducing data in it whereas in R it’s a doddle2 so I exported the R data frame I created and then imported it into SQL. It did mean that I could double check the final counts were the same which was useful!\r\nThe data warehouse that I use has a look up table of dates from 1900 until well into the future, with a single row of observations for each day. Other information is in there like, what was the day of the week, what financial year was it, is it today? But in the absence of such a table you will need to create a table with dates like so3:\r\n\r\n--Create date sequence in SQL using the first start date and the last end date from the data, \r\n--in this case I saved it in the SQL table under my own schema ZT and called the table Data\r\nDECLARE @startDate date = (SELECT MIN(start_date) FROM ZT.Data),\r\n    @endDate date = (SELECT MAX(end_date) FROM ZT.Data);\r\n \r\nSELECT DATEADD(day, number - 1, @startDate) AS [Date]\r\nINTO #calendar\r\nFROM (\r\n    SELECT ROW_NUMBER() OVER (\r\n        ORDER BY n.object_id\r\n        )\r\n    FROM sys.all_objects n\r\n    ) S(number)\r\nWHERE number <= DATEDIFF(day, @startDate, @endDate) + 1;\r\n\r\nFill in the dates between start and end date\r\nAlso adding in the floored dates for month and year4\r\n\r\nSELECT *\r\n,DATEADD(MONTH,DATEDIFF(MONTH,0,date),0) AS month_year\r\n,DATEADD(YEAR,DATEDIFF(YEAR,0,date),0) AS [year]\r\nINTO #extended\r\nFROM ZT.[data]\r\nINNER JOIN #calendar AS cal ON [start_date] <= cal.date AND [end_date] > = cal.date\r\n\r\nCount the observations\r\nTo match the previous counts in R where the individual referrals by patient were counted in a period, in SQL you have to merge the two data together because DISTINCT(patient_id, referral_id) won’t work\r\nBy month and year\r\n\r\nSELECT month_year\r\n,COUNT(DISTINCT CONCAT(patient_id, referral_id))\r\nFROM #extended\r\nGROUP BY month_year\r\nORDER BY month_year\r\n\r\nBy year\r\n\r\nSELECT [year]\r\n,COUNT(DISTINCT CONCAT(patient_id, referral_id))\r\nFROM #extended\r\nGROUP BY [year]\r\nORDER BY [year]\r\n\r\nLooking for a fixed period\r\nIf you only want one period of time and don’t want, or need, to create the calendar lookup counts can be done for ‘being open’ in the WHERE clause5:\r\n\r\nDECLARE @start_period datetime2 = '2020-12-01 00:00:00.0000000'\r\nDECLARE @end_period datetime2 = '2021-12-31 00:00:00.0000000'\r\n\r\nSELECT COUNT(DISTINCT CONCAT(patient_id, referral_id))\r\nFROM ZT.data\r\nWHERE ((end_date >= @start_period) AND [start_date] <= @end_period)\r\n\r\nOr more explicitly:\r\n\r\nDECLARE @start_period datetime2 = '2020-12-01 00:00:00.0000000'\r\nDECLARE @end_period datetime2 = '2021-12-31 00:00:00.0000000'\r\n\r\nSELECT COUNT(DISTINCT CONCAT(patient_id, referral_id))\r\nFROM ZT.data\r\n--Started before period and ended after period or still open (Patient 6)\r\nWHERE (([start_date] < @start_period AND (end_date > = @end_period)) \r\n--Started and ended in period (Patient 3) and (Patient 2)\r\nOR ([start_date] < = @end_period AND end_date > = @start_period) \r\n--Started in period and ended after period or still open (Patient 4)\r\nOR ([start_date]> = @start_period AND (end_date > = @end_period)))\r\n\r\nAny other ways?\r\nThis is possibly something analysts across the NHS have solved so I’d be really interested in hearing about any other ways of creating caseloads or counting open referrals in a given period.\r\n\r\nThe link no longer works but I’ve emailed AphA about the document which was here https://www.aphanalysts.org/wp-content/uploads/2016/08/JOIS_2016_038_Diagnosing_the_Flow_Constraint_i.pdf↩︎\r\nInformal British meaning very easy to do↩︎\r\nFrom a blog by Andrey Zavadskiy↩︎\r\nFrom Stackoverflow↩︎\r\nThe code for this WHERE clause was shared with me from a previous colleague, Barney Lawrence whose blog and Twitter are worth checking out for more SQL information.↩︎\r\n",
    "preview": "posts/2021-05-02-creating-a-caseload-over-time-in-sql/img/pink-tulip.jpg",
    "last_modified": "2021-05-02T18:12:17+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-01-creating-a-caseload-over-time-in-r/",
    "title": "Creating a caseload over time in R",
    "description": "How to count referrals/patients/anything between two dates in a given period of time.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-05-01",
    "categories": [
      "Generate data",
      "R"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Photo of dark pink tee blossom\r\n\r\n\r\n\r\nThe puzzle\r\nMy wonderful colleague Lori asked a question on some analysis that I hadn’t really considered for a long time but I had spent an inordinate amount of time solving a few years ago. She wanted to assess caseload1 over time to see if things had changed.\r\nThe logic\r\nThe logic of finding people is summed up nicely here:\r\n\r\n2\r\nIt covers all the scenarios of a particular time period:\r\nWho was open before the period and still open in the period\r\nWho was open and closed in the period\r\nWho was open in the period but closed outside of the period\r\nWho was open before the period and closed after the period\r\nAnd needs to exclude those who are:\r\nOpen and closed before the period\r\nOpen and closed after the period\r\nIf the period of time you are looking for is repeated, so over several years or several months, thoughts may turn to some sort of loop to repeatedly count the people.\r\nStop!\r\nThe secret\r\nIt’s all in the dates. The solution is to create an observation, a row essentially, for every date between the start and end dates. This then offers the flexibility to count by any period.\r\nWord of warning\r\nNote that tables can be huge if you are working with large datasets which cover long periods of time and are creating observations at the day level rather than month or year. Even so, R coped well with a dataset that had 18.5k over several years, ending up as 5,5 million observations. It took about 30 seconds to complete but if you find things are unbearably slow the options are either to increase RAM capacity, use the power of a SQL server, or the other options may be to recode using other packages like data.table.\r\nCode solution\r\nCreate some data\r\n\r\n\r\nlibrary(lubridate)\r\nlibrary(tidyverse)\r\n\r\nset.seed(130) # so the numbers generated will replicate\r\n\r\n# create random start and end dates and ids which can be repeated\r\ndata <- data.frame(\r\n  start_date = sample(seq(as.Date('2019/01/01'), as.Date('2021/01/01'), by = \"day\"), 300),\r\n  end_date = sample(seq(as.Date('2019/01/01'), as.Date('2021/01/01'), by = \"day\"), 300),\r\n  patient_id = floor(runif(300, min = 1, max = 300))\r\n)\r\n\r\n# Add a referral_id which is realistic and also because a patient_id can have multiple dates generated like patient_id 10 for example\r\n\r\ndata_filtered <- data %>% \r\n  filter(end_date > start_date) %>% \r\n  group_by(patient_id) %>% \r\n  mutate(referral_id = row_number(start_date))\r\n\r\n\r\n\r\nFill in the dates between start and end date\r\nThe function that gets all the dates betweeen the start and end date is complete() from the tidyr package (part of tidyverse). In this case the code is creating a sequence using seq.Date and filling in by day. This could be by month or by year but for this example it’s by day as it’s not too big a dataset and gives greater flexibility on the later counts which are by month and then by year.\r\n\r\n\r\n    # Create an observation for every date between the start and end date \r\n    data_expanded <- data_filtered %>%\r\n      group_by(patient_id,\r\n               referral_id) %>% \r\n      pivot_longer(cols = ends_with(\"date\"),\r\n                   names_to = \"caseload\",\r\n                   values_to = \"dates\") %>% \r\n      complete(dates = seq.Date(min(dates), max(dates), by=\"day\")) %>% \r\n      ungroup() # affects any counts or summarising later\r\n\r\n\r\n\r\nCount the observations\r\nBy month and year\r\n\r\n\r\ndata_expanded %>% \r\n  mutate(month_year = lubridate::floor_date(dates, \"1 month\")) %>% \r\n  group_by(month_year) %>% \r\n  summarise(count = n_distinct(patient_id, referral_id))\r\n\r\n\r\n# A tibble: 25 x 2\r\n   month_year count\r\n   <date>     <int>\r\n 1 2019-01-01    14\r\n 2 2019-02-01    25\r\n 3 2019-03-01    38\r\n 4 2019-04-01    48\r\n 5 2019-05-01    57\r\n 6 2019-06-01    68\r\n 7 2019-07-01    75\r\n 8 2019-08-01    78\r\n 9 2019-09-01    80\r\n10 2019-10-01    78\r\n# ... with 15 more rows\r\n\r\nBy year\r\n\r\n\r\ndata_expanded %>% \r\n  mutate(year = lubridate::year(dates)) %>% \r\n  group_by(year) %>% \r\n  summarise(count = n_distinct(patient_id, referral_id))\r\n\r\n\r\n# A tibble: 3 x 2\r\n   year count\r\n  <dbl> <int>\r\n1  2019   120\r\n2  2020   116\r\n3  2021     1\r\n\r\n\r\nalso known as open referrals but this can be anything with a start and end date↩︎\r\nThe link no longer works but I’ve emailed AphA about the document which was here https://www.aphanalysts.org/wp-content/uploads/2016/08/JOIS_2016_038_Diagnosing_the_Flow_Constraint_i.pdf↩︎\r\n",
    "preview": "posts/2021-05-01-creating-a-caseload-over-time-in-r/img/dark-pink-blossom.jpg",
    "last_modified": "2021-05-01T20:18:26+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-18-compare-sql-and-r-code/",
    "title": "Compare SQL and R code",
    "description": "Generating sequential numbers code in SQL compared to R",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-02-18",
    "categories": [
      "SQL",
      "R",
      "Generate data"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Close up of lichen on a tree branch\r\n\r\n\r\n\r\nGenerating data\r\nWhen I was preparing a talk for the 2020 NHS-R Community conference on Why do I need to learn R when I can use SQL? I realised that I had never really created any data in SQL like I was doing in R. It was quite fun writing out the code to generate data in both so today, as I was going through old files and tidying them up, I found some SQL code to generate sequential numbers and I wanted to put it somewhere. This seems a good place to put it and also take the opprtunity to compare it to R.\r\nGenerate sequential numbers in SQL\r\nGenerate numbers 1 to 20 sequentially in SQL1.\r\n\r\n\r\n;WITH CTE AS  \r\n(  \r\n SELECT 1 AS number  \r\n \r\n UNION ALL  \r\n \r\n SELECT number + 1 \r\n FROM CTE \r\n WHERE number <20  \r\n)  \r\n  \r\nSELECT * \r\nFROM CTE \r\n\r\nCompare to R\r\n\r\n\r\nseq(1:20)\r\n\r\n\r\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\r\n\r\nGenerate random numbers SQL\r\n\r\n\r\n-------------------------------------\r\n-- Generate a set of random numbers\r\n-- (values between 0 and 1000)\r\n-------------------------------------\r\nWITH RandomNumbers (RowNumber, RandomNumber) AS (\r\n    -- Anchor member definition\r\n    SELECT  1                         AS RowNumber, \r\n            RAND( CHECKSUM( NEWID())) AS RandomNumber\r\n    UNION ALL\r\n    -- Recursive member definition\r\n    SELECT  rn.RowNumber + 1          AS RowNumber, \r\n            RAND( CHECKSUM( NEWID())) AS RandomNumber\r\n    FROM RandomNumbers rn\r\n    WHERE rn.RowNumber < 10\r\n)\r\n-- Statement that executes the CTE\r\nSELECT  rn.RowNumber                     AS RowNumber,\r\n        ROUND(rn.RandomNumber * 1000, 0) AS RoundedRandomNumber\r\nFROM RandomNumbers rn;\r\nGO\r\n\r\nThanks to https://www.codeproject.com/Tips/811913/Generating-a-set-of-random-numbers-in-SQL-Server\r\nGenerate random numbers R\r\n\r\n\r\nsample(seq(1:100), 20)\r\n\r\n\r\n [1] 21 89 26 71 28 13 73 61 23 92 43  2 41 63 74 84 82 51 11 54\r\n\r\nConclusion\r\nI couldn’t have typed out the SQL code from memory and although I had a fair idea that I needed to still use the CTE and restrict by an additional row_number, I gave in easily and looked up code to do this. All in all, it was going to take more thinking time than I wanted to give it and, compared to R… well, there is no comparison. It’s a memorable short piece of base R code.\r\n\r\n\r\n\r\n\r\nTo run this will require a connection to a SQL server↩︎\r\n",
    "preview": "posts/2021-02-18-compare-sql-and-r-code/img/lichen-on-branch.jpg",
    "last_modified": "2021-02-18T16:37:16+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-13-finding-sql-varcharmax/",
    "title": "Finding SQL varchar(max)",
    "description": "SQL code to find the columns of max that need to be moved to the end of code for importing to R",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-02-13",
    "categories": [
      "SQL",
      "Error in R"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Photo of tree branches in silohette\r\n\r\n\r\n\r\nImport problems\r\n\r\n\r\n\r\nFigure 2: Tweet: Dates stored as varchar is the SQL equivalent of ‘red pants in the white wash’\r\n\r\n\r\n\r\n@_JohnMackintosh posted in Twitter about his ‘red pants in the wash’ being those date columns that are in fact varchar(). I’ve been very lucky as our data warehouse people have consistently formatted dates in the SQL tables including my favourite integer yyyymmdd format which beautifully imports into R with no issues.\r\nWhat I have found though, which is my ‘red sock in the wash’, is varchar(max) or nvarchar(max) data which, it seems, causes an import error unless these columns are placed at the end of the select script.\r\nMy sock\r\nThe first time I noticed this, it was because of a table I had built myself. I had put in a column for comments and naturally went for varchar(max). To solve the problem I just changed the data type for the column - because I could.\r\nOthers’ socks\r\nThe next time I encountered this, which wasn’t too long after, it was in a database I couldn’t update so I found my import scripts to R failing with an error:\r\n\r\nError in result_fetch(res@ptr, n) : nanodbc/nanodbc.cpp:3011: 07009: [Microsoft][ODBC SQL Server Driver]Invalid >Descriptor Index Failed to execute SQL chunk\r\n\r\nwhich isn’t really descriptive but searching for the error brought this up for the ODBC package:\r\n\r\n\r\n\r\nFigure 3: GitHub Issue: Short story is this is a bug in the MIcrosoft Driver with varchar(max) columns and they need to be at the end of the select query to work.\r\n\r\n\r\n\r\nFinding the socks\r\nSome of the tables I was working with have nearly 100 columns and whilst I can scan through for them, I’ve recently had to import about 20 tables so this would be long-winded.\r\nFirst, find your data type number:\r\n\r\n\r\n-- USE database_example\r\n\r\nSELECT system_type_id, name\r\nFROM sys.types\r\nWHERE system_type_id = user_type_id\r\n\r\nThen when you know what varchar(max) or even nvarchar(max) is listed under for your own database search for it with this:\r\n\r\n\r\n-- USE database_example\r\n\r\nSELECT\r\n    SchemaName = s.name\r\n    ,o.type_desc\r\n    ,ObjectName = o.name\r\n    ,ColumnName = c.Name\r\nFROM sys.objects AS o\r\nLEFT JOIN sys.schemas AS s ON o.schema_id = s.schema_id\r\nLEFT JOIN sys.all_columns AS c ON o.object_id = c.object_id\r\nWHERE c.system_type_id IN (231, 167)\r\nAND c.max_length = -1\r\n\r\n--Other useful code to restrict what is returned\r\nAND s.name IN ('schema_name') -- restrict to the schema\r\nAND o.type_desc = 'VIEW' -- or USER_TABLE \r\nAND o.name LIKE '%table_name%' -- or use = 'precise_table_name'\r\n\r\nMoving the socks\r\nUnfortunately, now I can’t use the really straight forward script:\r\n\r\n\r\nSELECT *\r\nFROM schema.Table\r\n\r\nand instead I have to list out all 100 columns and move those that are varchar(max) to the end. Still, this code saves lots of peering at the screen.\r\n\r\n\r\n\r\nFigure 4: Happy bouncing blue and white sock\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-13-finding-sql-varcharmax/img/tree-branch-silohette.PNG",
    "last_modified": "2021-02-13T15:30:54+00:00",
    "input_file": {},
    "preview_width": 2220,
    "preview_height": 2400
  },
  {
    "path": "posts/2021-02-08-using-monstr-package/",
    "title": "Using the monstR package",
    "description": "Gettting regional deaths from the monstR package",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-02-08",
    "categories": [
      "ONS data",
      "R"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Photo of snowdrops\r\n\r\n\r\n\r\nMoving away from messy solutions\r\nI’ve been using weekly provisionally recorded deaths from ONS for a number of years and it started off with getting the data directly and copying, by hand (the horror!), the one bit I needed from the East Midlands into a long table which had been started by a colleague. This was the easiest solution at the time but, as this these things often go, the easiest solution quickly becomes the hardest to maintain.\r\nVarious iterations\r\nThe code I’ve used has various iterations with the first being used for the ons_mortality dataset that is included in the {NHSRdatasets} package. The intention was always for it to be a training dataset, not a reporting dataset, so the data only cover 2010 to 2019 and I would top up the data with newly released information from ONS by running the code used to build the {NHSRdatasets} ons_mortality table for weeks as they were released.\r\nI wrote a vignette on how I built the ons_mortality dataset to show the approach I took to building the data. As my coding in R has improved, I’ve since adapted that code to include:\r\nfunctions to clean the data\r\ncode to scrape from the website where the name changes (each week there is a new url)\r\nfunctions to extract all the tabbed sheets from the downloads and\r\nfunctions to load multiple csvs\r\nONS API\r\nThe code was based on the ever-changing weekly urls throughout 2020, and towards the end of January 2021 I realised that I had muddled the data as I was getting 2021 spreadsheets and calling them 2020.\r\nGoing back over this code, with all it’s intricate cleaning code, was a bit daunting so I thought this would be a perfect time to have a go with the Health Foundation ONS API package called (fantastically!) {monstR}.\r\nWhat I discovered\r\nFirst of all I tried this function: ons_available_datasets() and I got a lot of information in the console. The text was overwhelming and I wasn’t sure what I was looking at. I was just about to write an issue question (the contributors are lovely and have labelled a previous issue of mine as a question and answered it) when I had the breakthrough thought of just popping it into an object and lo, it is actually a data frame that I was looking at in the console.\r\n\r\n\r\nlibrary(monstR)\r\n\r\ndf <- ons_available_datasets()\r\n\r\nview(df)\r\n\r\n\r\n\r\nThe next bit was to look for the data I wanted specifically and, luckily for me, the vignette already refers to it: weekly-deaths-region. I wasn’t sure what all the functions/verbs are doing but the notes said a folder is created.\r\n\r\n\r\nmonstr_pipeline_defaults() %>%  # Uses the monstr 'standards' for location and format\r\n  ons_datasets_setup() %>% \r\n  ons_dataset_by_id(\"weekly-deaths-region\") %>%\r\n  ons_download(format=\"csv\") %>%\r\n  monstr_read_file() %>%  \r\n  monstr_clean() %>%\r\n  monstr_write_clean(format=\"all\")\r\n\r\n\r\n\r\nI searched for this folder for ages. The text output doesn’t say exactly where it goes:\r\n\r\nINFO [2021-02-08 21:18:54] Edition not specified, defaulting to latest version INFO [2021-02-08 21:18:54] Retrieving dataset metadata from https://api.beta.ons.gov.uk/v1/datasets/weekly-deaths-region/editions/covid-19/versions/16 INFO [2021-02-08 21:18:55] Downloading data from https://download.beta.ons.gov.uk/downloads/datasets/weekly-deaths-region/editions/covid-19/versions/16.csv INFO [2021-02-08 21:18:55] File created at /data/raw/ons/weekly-deaths-region/covid-19/weekly-deaths-region-v16.csv\r\n\r\n\r\n– Column specification >—————————————————————— cols( V4_1 = col_double(), Data Marking = col_character(), calendar-years = col_double(), Time = col_double(), administrative-geography = col_character(), Geography = col_character(), week-number = col_character(), Week = col_character(), recorded-deaths = col_character(), Deaths = col_character() )\r\nINFO [2021-02-08 21:18:55] Writing csv data to >/data/clean/ons/weekly-deaths-region/covid-19/weekly-deaths-region-v16.csv\r\nINFO [2021-02-08 21:18:55] Writing xlsx data to >/data/clean/ons/weekly-deaths-region/covid-19/weekly-deaths-region-v16.xlsx\r\nINFO [2021-02-08 21:18:55] Writing rds data to >/data/clean/ons/weekly-deaths-region/covid-19/weekly-deaths-region-v16.rds [1] TRUE\r\n\r\nAnd as a search on Windows 10 files takes ages I couldn’t find the folder. After hours (yes, I’m not sure why it took that long either!) I eventually found it by closing down the project and just saving from default R Studio window. From here I got the folder to load:\r\n\r\npathway <- \"C:\\data\\clean\\ons\\weekly-deaths-region\\2010-19\"\r\n\r\nBut when searching for weekly-deaths-region I had found a previous download in a project folder so I’m a bit baffled how it works. I’ve raised an issue on the GitHub to ask for folder pathway to be highlighted or just have the data be loaded as an object rather than saving, which is my preferred option particularly as the files structure is so long: /data/clean/ons/weekly-deaths-region/covid-19/.\r\nLooking at the data\r\nAfter loading the data (I chose the .rds file to load but there also an .xlsx and a .csv) I filtered it to the area I wanted:\r\n\r\n\r\nreadRDS(\"C:/data/clean/ons/weekly-deaths-region/2010-19/weekly-deaths-region-v1.rds\")\r\n\r\nmortality <- `weekly-deaths-region-v1` %>% \r\n  filter(geography == \"East Midlands\")\r\n\r\n\r\n\r\nThe data only has data from 2010 to 2019 (given that’s the file name it’s not surprising but I missed that!) and makes sense as 2020 was when the data output changed to take into account Covid-19 deaths. There are other editions and the issue/question I mentioned before was answered by Emma Vestesson with an extra bit of code specifying editions.\r\n\r\n\r\n# find 'editions'\r\n\r\nons_available_editions(\"weekly-deaths-region\") \r\n\r\n\r\n\r\nPrints to console\r\n2010-19 - what I’d already downloaded\r\ncovid-19 - what I need\r\nThe code to get 2020 and currently added deaths is:\r\n\r\n\r\nmonstr_pipeline_defaults() %>%  # Uses the monstr 'standards' for location and format\r\n  ons_datasets_setup() %>% \r\n  ons_dataset_by_id(\"weekly-deaths-region\", edition = \"covid-19\") %>% #<<\r\n  ons_download(format=\"csv\") %>%\r\n  monstr_read_file() %>%  \r\n  monstr_clean() %>%\r\n  monstr_write_clean(format=\"all\")\r\n\r\n\r\n\r\nUsefully, and perhaps why this is saved to folders rather than loaded as objects, this creates a folder in the same area as the other code: C:/data/ons/weekly-deaths-region/covid-19 and here I get v16 and v17 data.\r\n\r\n\r\nreadRDS(\"C:/data/clean/ons/weekly-deaths-region/covid-19/weekly-deaths-region-v16.rds\")\r\n\r\nreadRDS(\"C:/data/clean/ons/weekly-deaths-region/covid-19/weekly-deaths-region-v17.rds\")\r\n\r\n\r\n\r\nAgain, filtering down to make it easier to see what data I have for the region I’m looking at as “East Midlands” and also the recorded_deaths as “total-registered-deaths” as this data includes “deaths-involving-covid-19-occurrences” which, in this case, I don’t want:\r\n\r\n\r\nmortality_v16 <- `weekly-deaths-region-v16` %>% \r\n  filter(geography == \"East Midlands\",\r\n         recorded_deaths == \"total-registered-deaths\")\r\n  \r\nmortality_v17 <- `weekly-deaths-region-v17` %>% \r\n  filter(geography == \"East Midlands\",\r\n         recorded_deaths == \"total-registered-deaths\")\r\n\r\n\r\n\r\nTwo versions\r\nBoth have 2020 and 2021 data and return the same number of obs. It may be that something has changed in the versions in a different area of the data as when I use the package {dataCompareR} I confirmed this data extraction is exactly the same:\r\n\r\n\r\nlibrary(dataCompareR)\r\n\r\n# This needs a common identifier, which there isn't one in the objects so I've ordered and then added a row_number:\r\n\r\nmortality_v16 <- `weekly-deaths-region-v16` %>% \r\n  filter(geography == \"East Midlands\",\r\n         recorded_deaths == \"total-registered-deaths\") %>% \r\n  arrange(week_number,\r\n          calendar_years) %>% \r\n  mutate(rn = row_number())\r\n  \r\nmortality_v17 <- `weekly-deaths-region-v17` %>% \r\n  filter(geography == \"East Midlands\",\r\n         recorded_deaths == \"total-registered-deaths\") %>% \r\n  arrange(week_number,\r\n          calendar_years) %>% \r\n  mutate(rn = row_number())\r\n\r\n# dataCompareR code\r\n\r\nCompared <- rCompare(mortality_v16, mortality_v17, key = \"rn\")\r\n\r\n# I like to save this because I still have to get my head around using lists and this function saves and opens an html at the same time\r\n\r\nsaveReport(Compared, reportName = \"comparing\")\r\n\r\n\r\n\r\nIt makes sense to use the latest version, just in case, and so I need to add the 2010-2019 data for recorded_deaths to the 2020-2021 v17 data:\r\n\r\n\r\n# code copied from earlier chunks to explain the process so it's all in one area\r\n\r\nmortality_v1 <- `weekly-deaths-region-v1` %>% \r\n  filter(geography == \"East Midlands\")\r\n\r\nmortality_v17 <- `weekly-deaths-region-v17` %>% \r\n  filter(geography == \"East Midlands\",\r\n         recorded_deaths == \"total-registered-deaths\",\r\n         !is.na(v4_1)) # not all dates have counts just like the website spreadsheets\r\n\r\nmortality_all_years <- mortality_v1 %>% \r\n  rbind(mortality_v17)\r\n\r\n\r\n\r\nInterestingly, at the time of writing the latest date was week 1 of 2021 but I am writing in week 4 so there appears to be a delay in this data.\r\nThe value of APIs\r\nAPIs are a great way of getting data as it’s from the source and can be updated according to any changes that source holder makes. The difficulty with them is they have their own technological language but R packages are a nice way around that if you are familiar with R.\r\nI think the package looks very promising and could do with more vignettes as I’m pretty sure I wouldn’t have got this far with it if I were very early on in using R. Saying that, contribution is really easy as the Health Foundation team are great at paving the way for open source working.\r\nOther links\r\nThe weekly provisional deaths are published here also monthly deaths at a lower regional level is available here. These are not provisional deaths so can be useful for more accurate tracking but the format of the spreadsheets has changed considerably over time. This was my attempt at tidying it.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-08-using-monstr-package/img/snowdrops.jpg",
    "last_modified": "2021-02-13T15:22:00+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-05-creating-sample-datetime-data-in-r/",
    "title": "Creating sample datetime data in R",
    "description": "How to produce a reproducible example (reprex) for datetime",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-02-05",
    "categories": [
      "R",
      "Generate data"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Frosty leaves\r\n\r\n\r\n\r\nThe product of my drifting mind\r\nI was writing a blog for my team’s blog about working in the open and midway I drifted into writing code. For some reason I had a burning desire to highlight a common issue in SQL with the use of BETWEEN for dates and then ended up spending working out how to create fake date/time data in R.\r\nReading through my team blog I realised it didn’t make sense to have the R data creation code in it so I’ve promoted it to its own blog - this! The SQL thing with BETWEEN will also get its own blog in due course.\r\nReproducible examples\r\nI like producing reprexes1 and I have a few gists where I’ve answered some questions from places like Stackoverflow and R Studio Community by first creating reprexes. These were questions which were removed, unfortunately, before I had a chance to send the reply. One time I had an answer, with a reprex, in just 20 minutes and was about to post, but it had been deleted! I wasn’t prepared to lose that code so I posted it in my own GitHub gist and I’ve used the reprex code many times since.\r\nReproducing what?\r\nI was trying to recreate the SQL date time format YYYY-MM-DD hh:mm:ss[.nnn] that I have in my work’s data warehouse. For this example I’ve only reproduced a random sample in the SmallDateTime YYYY-MM-DD hh:mm:ss format.\r\nLosing the code\r\nI spent a long time trying to work out how to randomly sample the constituent parts of a date time (including hours, minutes and seconds), even using base R code and the chron package to get hm (but not s)2.\r\nBy the end of the day I was very tired and I did a terrible thing: I cut the code from an Untitled and unsaved file, moved to another R project and then copied something else. I didn’t stop there. Oh no, I then copied and pasted several things then, when I finally went to paste what I’d cut originally it was, of course, gone.\r\n\r\n\r\n\r\nWindows history clipboard\r\nTurns out Windows 10 has a clipboard history but you have to switch it on by going to Windows settings/Clipboard settings and switch on history.\r\nI looked for a GIPHY for “nice to know” but none conveyed the right level of sarcasm for that phrase.\r\nEvery cloud has a silver lining and all that…\r\nFrantically re-writing code after deleting huge swathes means that you do get an opportunity to improve the code. At least, that’s what I told myself. And so I re-wrote what I could remember and realised I’d missed the seconds and also how I’d not even checked the {lubridate} package which does indeed produce sequential dates and time which can be sampled:\r\n\r\n\r\nlibrary(lubridate)\r\n\r\nlubridate_dhms <- data.frame(\r\n  hms = sample(seq(ymd_hms(\"2020-1-1 0:00:00\"), ymd_hms(\"2021-1-1 0:00:00\"), \r\n                    by = \"hour\"), 15)\r\n)\r\n\r\n\r\n\r\nBut the by = \"\" only accepts hour, not minute or second so those are all 00:00.\r\nHelp!!!!\r\nBy this point I was a bit fed up so I did what all good coders who have exploited the internet for help do - I asked my NHS-R Community colleagues on Slack:\r\n\r\nIs there any way to generate random hours, minutes and seconds for made-up data?\r\n\r\nAnd I included examples of what I’d attempted.\r\nThus ensued a great thread with my boss, Chris Beeley, who answered the question within minutes.\r\n\r\n\r\nbase_r_dhms <- data.frame(\r\n  sample(seq(as.POSIXlt(\"2020-10-01\"),\r\n      as.POSIXlt(\"2020-10-10\"), by = 1), 15)\r\n)\r\n\r\n\r\n\r\nI asked what the by = 1 means and Chris confirmed this was a 1 second interval so the seq(…) creates the sequence at 1 seconds and then the sample() takes, in this case, 15 data points from this sequence.\r\nIt’s also possible to write “s” or “sec” in place of the 1:\r\n\r\n\r\nbase_r_dhms_same <- data.frame(\r\n  sample(seq(as.POSIXlt(\"2020-10-01\"),\r\n      as.POSIXlt(\"2020-10-10\"), by = \"s\"), 15)\r\n)\r\n\r\n# or\r\n\r\nbase_r_dhms_same <- data.frame(\r\n  sample(seq(as.POSIXlt(\"2020-10-01\"),\r\n      as.POSIXlt(\"2020-10-10\"), by = \"sec\"), 15)\r\n)\r\n\r\n\r\n\r\nBecause I’d also asked about {lubridate} generating random minutes and seconds, and Chris was having too much fun with this he answered that too:\r\n\r\n\r\n# using the code I shared that generates random dates and hours\r\n\r\nhour_min_sec <- data.frame(\r\n  hms = seq(ymd_hms(\"2020-1-1 0:00:00\"), ymd_hms(\"2021-1-1 0:00:00\"), \r\n                    by = \"hour\")\r\n)\r\n\r\n# updating the data frame with random seconds and updating the data\r\nlubridate::second(hour_min_sec$hms) <- sample(0 : 59, nrow(hour_min_sec), replace = TRUE)\r\n\r\n# updating the data frame with random minutes and updating the data\r\nlubridate::minute(hour_min_sec$hms) <- sample(0 : 59, nrow(hour_min_sec), replace = TRUE)\r\n\r\n\r\n\r\nHappy ending\r\nIn response to my saying:\r\n\r\nYou won’t believe how long I’ve been working on this and how many lines of code I have written!\r\n\r\nChris said:\r\n\r\nI bet you learned a lot though\r\n\r\nAnd it’s true.\r\n\r\nReproducible example or data that has been copied or made up to help explain a problem in data↩︎\r\nI used this blog http://datacornering.com/how-to-generate-time-intervals-or-date-sequence-in-r/↩︎\r\n",
    "preview": "posts/2021-02-05-creating-sample-datetime-data-in-r/img/leaves-frost.png",
    "last_modified": "2021-02-18T16:55:15+00:00",
    "input_file": {},
    "preview_width": 2997,
    "preview_height": 3808
  },
  {
    "path": "posts/2021-01-31-adding-disqus-to-distill-blogs/",
    "title": "Adding Disqus to distill blogs",
    "description": "Notes on adding comments to distill pages.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-01-31",
    "categories": [
      "Distill"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Photo of frost on wood\r\n\r\n\r\n\r\nInspiration\r\nWhen searching for “distill blog doesn’t show xaringan slides” I stumbled across this blog by Shamindra Shrotriya where I found that it’s possible to add comments to the distill pages.\r\nIssues with images\r\nI’ve never used Disqus before but I followed Shamindra’s blog instructions and it worked. The only thing to note is that images must be in PNG format. Thankfully I’d only included a few so I could convert them by opening them in Microsoft Paint and saving them as .PNG.\r\nI saved as .PNG rather than .png as @tomjemmett had had to set my files to this in the NHS-R Community GitHub to get the xaringan slides there to render. I thought it best to follow this as good practice.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-31-adding-disqus-to-distill-blogs/img/frost-on-wood.png",
    "last_modified": "2021-01-31T22:33:16+00:00",
    "input_file": {},
    "preview_width": 4000,
    "preview_height": 3000
  },
  {
    "path": "posts/2021-01-15-data-ethics/",
    "title": "Data Ethics",
    "description": "Philosophers are experts in their field",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [
      "Philosophy",
      "Ethics"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Photo of holly\r\n\r\n\r\n\r\n20 years after I graduated, I returned to my university to see how things had changed. The buildings hadn’t changed but it was a different world inside, and none more striking than the most imposing building on the campus; the library. Now this is a library with some history as it is was at Hull University where Philip Larkin worked as the head librarian and whilst I still have a soft spot for Hull, Philip Larkin wasn’t so enamoured. He wrote:\r\n\r\n“I’m settling down in Hull all right. Every day I sink a little further.”\r\n\r\nThe library building is the most imposing building on the campus, grey and tall, but it has a fabulous view from the top floor. I’d spent a bit of time in the library, mainly around the not-actually-that-dusty philosophy books although I did once venture into a section that had books that touched on the occult.1\r\nIt was breathtaking how modern everything was inside Brynmor Jones library. Whilst the exterior hadn’t changed, there was now a glass fronted lift and comfortable seating areas and plenty of plug points you’d find in any respectable coffee shop. Gone, though, were the banks of computers necessary in my day as so few people owned one. There were a few computers dotted about but these were only to access the library catalogue. Shockingly, some areas, like Computer Science, had fewer books than I expected, I’m guessing, because so much can be accessed electronically.\r\nInevitably, I drifted to the philosophy section and, to my delight, I was greeted with rows upon rows of books. The first journal I picked up was a lucky find as it contained an article called: Why flatulence is funny and what was even better was that my little one was just at the right age to find the first paragraph utterly hilarious. We giggled loudly, disturbing the studious atmosphere. For your delectation here is the extract:\r\n\r\nToot. Pass gas. Break wind. Cut the cheese. Float an air biscuit. Burp from behind. Blow the brown horn. The backfire, bant, bucksnort, booty bomb, colon cologne, drifter, fanny bubble, gasser, gurgler, moon beam, nether belch, pants puffer, pooh tune, rip-snort, sphincter whistle, thunder dumpling, tush tickler, and trouser cough. These are synonyms for a bodily function that is as natural as breathing, eating, or sleeping. Yet unlike other physiological functions, the ‘flatus’ is a source of endless humor – perhaps more so than any other subject in human experience. But why are farts funny? That is a question that is both serious and philosophically interesting.\r\n\r\nPhilosophy is not a dusty old subject, although it’s often expected to be just that that. It’s a subject that has existed in its own right, as well as integrated into other subjects like religion and science, or with economics and psychology. I even had a lecturer who had studied maths and philosophy which, at the time, I thought was a strange combination. I realise now that philosophy is part of every subject in one way or another and makes sense to be combined with maths because of the overlap of logical and critical thinking.\r\nIndeed, philosophers like Bertrand Russell, who inspired me to choose philosophy after I read some of his work2, was also a fine mathematician. Along with Alfred North Whitehead, another mathematician, they wrote 360 pages to prove definitively that 1 + 1 = 23 in “Principia Mathematica”.\r\nDifferent ways of thinking\r\nI was lucky with my degree in that it was heavily weighted to the final year, so I was able to take modules in other courses without much fear of affecting my final degree class. I studied in 3 subject areas, history, physics and religion.\r\nThe history module went badly as I quickly realised; I don’t think like a historian. I had been trained in philosophy so I questioned the question, didn’t questioned the ‘facts’ and, I don’t know, just constantly posed questions which didn’t go down too well.\r\nThe second subject was a physics module designed to get the scientists to think about their place in society. I absolutely aced it because I was used to writing essays and it was wholly philosophical in its approach.\r\nThe final module was in religious studies and I had a lecturer who respected philosophy deeply and thought of himself as a philosopher. The essay question was also very close to my final dissertation (about morality) so I, sort of, plagiarised myself, if that’s technically possible.\r\nI’m not certain, but I do wonder if those two successful essays, the physics and the religious studies, would have been graded lower by my philosophy lecturers because, whilst the lecturers were rigorous in their marking, I did exceptionally well in them compared to my usual results.\r\nWhat was so interesting at that time, and which I appreciated very quickly given my poor mark in the history module, was that we fundamentally thought differently. It’s something that intrigues me even today as I can see how medics think differently to patients, analysts think differently to commissioners, and managers think differently to juniors.\r\nNew subject areas\r\nArguing is pretty fundamental to philosophy but it’s the way you argue that matters. There is a structure to it and an approach to subjects that is core to all areas of philosophy. But still, specialisms have changed.\r\nIn my day I was offered the chance to study metaphysics and ethics and I chose ethics but, in those early days of the internet and computing, there was never any talk (at least in my lectures) of data ethics. This is clearly a growing subject area with big data and Artificial Intelligence, so much so that the UK Government Digital Service even have a Data Ethics Team who, this month, have run a workshop to explore the skills a data ethicist should have.\r\nComplementing this they released a short survey for data ethics professionals (all sectors) aimed at identifying these skills. I was surprised that the first question, whilst comprehensively detailed, lacked a point blank tick box for philosophy and it is bundled in with other social science subjects:\r\n\r\nAccording to you, which of the following skills are the most important in data/AI ethics? Choose up to six. *\r\nAbility to quickly read and interpret complex documents from a range of sources and distill to what is relevant\r\nKnowledge of/ background in social science (anthropology, economics, sociology, philosophy etc)\r\nProblem-solving skills\r\nExperience translating technical information for a non-technical audience, and vice versa\r\nDomain knowledge of existing schools of thought and exemplar models of data ethics in practice\r\nCommunication skills\r\nData modelling, data cleansing, and data enrichment skills\r\nData literacy\r\nProject management skills\r\nNetworking and engagement skills; working collaboratively across multidisciplinary teams and stakeholder from data science and other areas (policy, social science)\r\nProven IT and mathematical skills\r\nBroad policy analysis and evaluation experience\r\nAbility to draw together, analyse, and critically evaluate information\r\nSound judgement on relevant information, stakeholders, and value-driven activities\r\nApplied maths, statistics, and scientific practices\r\nLogical and creative thinking skills\r\nUnderstanding analysis across the product life cycle\r\nQuality assurance, validation and data linkage abilities\r\nData engineering and manipulation\r\nOther:\r\n\r\nI liked some of these points as they look like they are breaking down what philosophical thought demands but none really felt like what I learned in my degree. Yes, I learned to quickly read and interpret complex documents from a range of sources and distill to what is relevant but I used the skills listed in the Times Higher Education to be able to do it:\r\n\r\nThe skills learned on a philosophy degree, including clear and analytical thinking, persuasive writing and speaking, innovative questioning and effective reasoning, give a solid foundation for entering the workforce and are beneficial in careers that require problem-solving and assessing information from various angles.\r\n\r\nBalancing domain knowledge with skill\r\nWorking in analysis, I see this quite often, where people are analytical thinkers combining their domain4 knowledge with analysis. The same with philosophy. After all, it’s a subject that everyone can do to some extent. But what is worrying is when those that specialise in those fields, the analysts and the philosophers, get sidelined because people forget that they are experts in their respective fields.\r\nMany people can get to grips with the intricacies of law, particularly when it affects them, but they are not trained lawyers and if you needed legal representation you would still go to the lawyer or solicitor. If you are working with statistics it’s a good idea to include a statistician or data scientist and if you need mental health support, you go to a clinical psychologist and not someone who has read a lot about psychology. But how many ethics committees or data ethics group actually include an ethicist or even something akin to a ‘professional philosopher’? Is it more important to have domain knowledge and just practice the philosophy or should it be the other way around?\r\nI don’t really have the answer but my inclination, having studied this subject, is that you need a practising ethicist. Someone steeped in the subject both within specialisms and who is aware and up to date with general philosophical principles. It’s worth remembering that philosophy is not a series of facts to learn and regurgitate; it’s a way of thinking. It demands that you are both a deep thinker and also ask naive questions - of yourself and of others. You need to be curious and comfortable with uncertainty. Not the quantifiable uncertainty that statistics offers, but the uncertainty you get when you can see both sides of the story (or many sides of the story as is often the case). Philosophy is a discipline that should not be overlooked as an entity in its own right, even though it’s one of the few subjects that combines so imperceptibly with others.\r\nI’d be interested to know and talk to any professional applied ethicists or any ethics committees with ethicists. My contact details can be found here.\r\n\r\nI had searched for the Devil of Loudon by Aldous Huxley but didn’t take it out and, to this day, still haven’t read it.↩︎\r\nI had read about how a table is predominately made up of space as the molecules are not that dense. Consequently, how can we say a table is really there and why don’t we just walk straight through walls?↩︎\r\nThis really puts into perspective the weakness of the twitter hate directed at @kareem_carr when he wrote a thread on interpreting 2 + 2 = 5. The issue was that this particular equation appeared in George Orwell’s book, 1984, and so was seen from the perspective of thought control and dismissing fundamental mathematics. The ‘arguments’ though were often, I felt, an excuse for abuse but Kareem’s good humoured and levelled responses transcended the vitriol.↩︎\r\nBy domain I mean subject area, so working in healthcare, my data knowledge is around the NHS and its structures.↩︎\r\n",
    "preview": "posts/2021-01-15-data-ethics/img/holly.PNG",
    "last_modified": "2021-01-31T21:52:19+00:00",
    "input_file": {},
    "preview_width": 3968,
    "preview_height": 1984
  },
  {
    "path": "posts/2020-12-19-publishing-xaringan-slides/",
    "title": "Publishing xaringan slides",
    "description": "Notes on how I published my NHS-R Conference lightning talk",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2020-12-19",
    "categories": [
      "Xaringan",
      "GitHub"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Chafffinch at bird tables\r\n\r\n\r\n\r\nPublishing xaringan slides\r\nThis is my first attempt to publish my NHS-R Community 2020 Virtual Conference Lightning talk slides using the code shared by Dr. Silvia Canelón who ran the workshop Sharing Your Work with xaringan. I’m indebted to Silvia’s workshop and also to the code she used to publish the workshop slides.\r\n\r\n\r\n\r\nUnfortunately, it looks like they render better on the index.Rmd file and not on an article. The slides look the same (with lost formats and no interactivity) whether the link is to the self-contained html in a repository on my computer or to the url.\r\n\r\n\r\n\r\nfitvids('.shareagain', {players: 'iframe'});\r\n\r\nThis RStudio Community conversation helped me realise this is probably something in distill and not something I was doing. In particular, it’s where @Apreshill says:\r\n\r\nBut also, this is the index.Rmd, not a post, so not entirely sure. I also added this line to my _site.yml, but confess to not testing whether that is required or not for this to work:\r\n\r\nI tried adding the line include: [“slides/”] @Apreshill refers to but that didn’t solve the formatting.\r\nWhy do I need to learn R when I can use SQL?  time: 1:05:07\r\nThings to note when publishing using GitHub\r\nTo get started there are lots of blogs on publishing to GitHub pages but this from R Studio/distill creators is also very clear.\r\nThe following are notes I’ve made and learned around publishing html and xaringan slides to GitHub.\r\nPublishing subfolders\r\nAlthough I managed to publish a GitHub repository following the instructions, I remained frustratingly confused about how to publish the html slides in subfolders. It turned out it was a simple thing that no one mentioned; you just need to add the subfolder directory to the url!\r\nFor example, @tomjemmett had published the NHS-R Community repository https://github.com/nhs-r-community/Conference_2020 and the url becomes: https://nhs-r-community.github.io/Conference_2020/\r\nTo see my xaringan slides I needed to get the page to the subfolder /Lightning_talks/ZoëTurner_SQLvR/ and this only needs to be added to the url like so:  https://nhs-r-community.github.io/Conference_2020/Lightning_talks/Zo%C3%ABTurner_SQLvR/index.html#1\r\nThe #1 at the end of the url refers to the slide number so you can go straight to a particular slide if you want.\r\nIt took me days of looking at websites on how to publish GitHub pages to realise this small thing and the danger of continued searching was that some pages referred to the old way of publishing GitHub pages which necessitated creating a new branch called gh-pages.\r\nThankfully, after days of puzzling over this I decided to take a copy (fork) of Silvia’s repository and run her code, which worked perfectly, so I knew I was on the right lines. Comparing the urls to the slides and where she she had saved them in her repository, I noticed the file paths were the same and then it all clicked for me. The point to all this is that it’s a generous thing to open up your code to such naive scrutiny and I am forever grateful to those that do.\r\nThings to note\r\nI have a special character in my name: Zoë. The ë appears like that in the url but when copied and pasted here, for example, it comes out as Zo%C3%AB. Thankfully, both work, which is good to know if you have an unusual character in your name!\r\nBe patient\r\nI quite often got a 404 error page just after saving a file to publish which was odd as everything worked ok. This was because it can take quite a few minutes for things to refresh.\r\nXaringan slides are blank\r\nAlso note that if the xaringan slides don’t appear but there is a blank box in the top left hand side of the screen and with a slider, it has worked. This can occur if you work on a VPN, as I do, where the security is very tight.\r\nI also noticed the same box when I knitted xaringan slides and I was using a link to a video. The particular slide would only work when the slides were opened in the Chrome browser. Given that the NHS often default to Explorer or Edge browsers, it might also be worth checking you are using Chrome if you are not on a VPN and still get the blank slider box.\r\nXaringan slides are not self-contained\r\nUnlike using html, the xaringan slides need to have the images, CSS and libraries available to run it. I made the mistake of using two folders for my images but it worked out that I used /img for the slide backgrounds and the /images for the pictures related only to that presentation.\r\nUpdate…\r\nSilvia kindly shared the update about making the xaringan presentations ‘self-contained’. I’d used that line of code for this blog originally as I hadn’t read the instructions and had wondered if that was why the slide navigation was missing (see the answer to that below!). Testing how ‘self-contained’ self-contained is I emailed myself just the html and got the text but no formats. Zipping together the html along with the css, img and libs (my names for these) folders did mean the html opened up ok with all the formatting. It’s a little like when saving a webpage and a same-named folder appears that is needed for the html to open.\r\nReasons not to use pdfs\r\nUsing pdfs gets around the need to email out supporting files and folders as it’s just one file to send, however, I’m reluctant to use pdfs because of accessibility as much as aesthetics:\r\nhttps://www.gov.uk/guidance/publishing-accessible-documents\r\n\r\nThink about format\r\nDoing this will help your document support as many users as possible and can future-proof your information.\r\nPublish in HTML format wherever possible so that your documents use your users’ custom browser settings. It can be difficult to make other formats easier to read.\r\nFor example, PDF documents:\r\ncan make your content harder to find, use and maintain do not work well with assistive technologies like screen readers a lot of the time. If your documents do not meet accessibility standards you could be breaking the Equality Act 2010.\r\n\r\nAesthetically, you lose interactivity (dygraphs or plotly charts) or movement in your slides (videos or GIFs) with pdfs so if you look through the commits in my Presentation repository you will see me flicking between html to pdf and back to html as I’ve tried this out.\r\nMissing slide navigation\r\nI was puzzled for a long time why I had published my slides in this blog but the slide navigation was missing. As I consistently fail to read instructions it turned out I needed the line:\r\n\r\n\r\nxaringanExtra::use_share_again()\r\n\r\n\r\n\r\nin the presentation slide code. I put this in the code chunk related to libraries (often listed as r libs).\r\nBecause I missed doing this from the beginning there were a few steps I needed to to to get this to publish on the blog. These were:\r\nPresentation repo\r\nAdd the line of code to the presentation code\r\nKnit the presentation\r\nCommit and push the presentation rmd and html file\r\nBlog repo\r\nKnit the blog post\r\nBuild the website\r\nCommit and push all changes\r\nDo a little celebration jig and thank Silvia yet again for helping me out as she let me know this missing line of code in the NHS-R Community Slack Group\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-19-publishing-xaringan-slides/distill-preview.png",
    "last_modified": "2021-01-31T21:45:22+00:00",
    "input_file": {},
    "preview_width": 453,
    "preview_height": 551
  }
]
